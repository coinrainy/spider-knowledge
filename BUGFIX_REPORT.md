# ğŸ”§ çˆ¬è™«ç³»ç»Ÿé—®é¢˜ä¿®å¤æŠ¥å‘Š

## ğŸ“‹ é—®é¢˜æ€»ç»“

### 1. é«˜çº§çˆ¬è™«é…ç½®é”™è¯¯
**é—®é¢˜**: `'target_sites'` é”®é”™è¯¯  
**åŸå› **: `crawler_manager.py` ä¸­åªä¼ é€’äº† `settings` éƒ¨åˆ†é…ç½®ç»™é«˜çº§çˆ¬è™«ï¼Œç¼ºå°‘ `target_sites` ç­‰å…³é”®é…ç½®  
**ä¿®å¤**: ä¼ é€’å®Œæ•´çš„ `advanced_crawler` é…ç½®å¯¹è±¡

```python
# ä¿®å¤å‰
config = self.config.get('advanced_crawler', {}).get('settings', {})

# ä¿®å¤å  
config = self.config.get('advanced_crawler', {})
```

### 2. åŸºç¡€çˆ¬è™«æ— æ³•è·å–æ–°é—»
**é—®é¢˜**: çˆ¬å–äº†0æ¡æ–°é—»  
**åŸå› **: ç½‘æ˜“æ–°é—»é¡µé¢ç»“æ„å˜åŒ–ï¼ŒåŸæœ‰é€‰æ‹©å™¨å¤±æ•ˆ  
**ä¿®å¤**: é‡å†™æ–°é—»é“¾æ¥æå–é€»è¾‘ï¼Œä½¿å…¶æ›´åŠ å¥å£®å’Œé€šç”¨

**æ”¹è¿›ç‚¹**:
- ä½¿ç”¨æ›´é€šç”¨çš„é“¾æ¥è¿‡æ»¤ç­–ç•¥
- æ·»åŠ æ–°é—»é“¾æ¥æœ‰æ•ˆæ€§æ£€æŸ¥
- æ”¹è¿›æ ‡é¢˜æå–é€»è¾‘
- å¢åŠ æ— æ•ˆå†…å®¹è¿‡æ»¤
- æ·»åŠ å®æ—¶è¿›åº¦æ˜¾ç¤º

## ğŸš€ ä»£ç è´¨é‡æ”¹è¿›å»ºè®®

### 1. é”™è¯¯å¤„ç†å¢å¼º
```python
# å»ºè®®æ·»åŠ æ›´è¯¦ç»†çš„å¼‚å¸¸å¤„ç†
try:
    # çˆ¬è™«é€»è¾‘
except requests.RequestException as e:
    logging.error(f'ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}')
except BeautifulSoup.ParserError as e:
    logging.error(f'HTMLè§£æå¤±è´¥: {e}')
except Exception as e:
    logging.error(f'æœªçŸ¥é”™è¯¯: {e}')
```

### 2. é…ç½®éªŒè¯
```python
def validate_config(self, config):
    """éªŒè¯é…ç½®æ–‡ä»¶å®Œæ•´æ€§"""
    required_keys = ['target_sites', 'settings']
    for key in required_keys:
        if key not in config:
            raise ValueError(f'é…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {key}')
```

### 3. æ—¥å¿—ç³»ç»Ÿä¼˜åŒ–
```python
# å»ºè®®ä½¿ç”¨ç»“æ„åŒ–æ—¥å¿—
import structlog

logger = structlog.get_logger()
logger.info('çˆ¬è™«å¯åŠ¨', crawler_type='basic', target_count=20)
```

### 4. æ€§èƒ½ç›‘æ§
```python
import time
from functools import wraps

def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        logging.info(f'{func.__name__} æ‰§è¡Œæ—¶é—´: {end-start:.2f}ç§’')
        return result
    return wrapper
```

### 5. æ•°æ®éªŒè¯
```python
from pydantic import BaseModel, validator

class NewsItem(BaseModel):
    title: str
    link: str
    summary: str = ''
    pub_time: str
    
    @validator('title')
    def title_must_not_be_empty(cls, v):
        if not v or len(v) < 5:
            raise ValueError('æ ‡é¢˜é•¿åº¦ä¸èƒ½å°‘äº5ä¸ªå­—ç¬¦')
        return v
```

### 6. ç¼“å­˜æœºåˆ¶
```python
from functools import lru_cache
import redis

# å†…å­˜ç¼“å­˜
@lru_cache(maxsize=1000)
def get_cached_page(url):
    return requests.get(url)

# Redisç¼“å­˜
class RedisCache:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    def get(self, key):
        return self.redis_client.get(key)
    
    def set(self, key, value, expire=3600):
        self.redis_client.setex(key, expire, value)
```

### 7. å¼‚æ­¥çˆ¬å–
```python
import asyncio
import aiohttp

class AsyncCrawler:
    async def fetch_page(self, session, url):
        async with session.get(url) as response:
            return await response.text()
    
    async def crawl_multiple(self, urls):
        async with aiohttp.ClientSession() as session:
            tasks = [self.fetch_page(session, url) for url in urls]
            return await asyncio.gather(*tasks)
```

### 8. å•å…ƒæµ‹è¯•
```python
import unittest
from unittest.mock import patch, MagicMock

class TestNewsCrawler(unittest.TestCase):
    def setUp(self):
        self.crawler = BasicNewsCrawler()
    
    @patch('requests.Session.get')
    def test_get_news_list(self, mock_get):
        # æ¨¡æ‹ŸHTTPå“åº”
        mock_response = MagicMock()
        mock_response.text = '<html><a href="/news/1">æµ‹è¯•æ–°é—»</a></html>'
        mock_get.return_value = mock_response
        
        news_list = self.crawler.get_news_list()
        self.assertGreater(len(news_list), 0)
```

### 9. é…ç½®ç®¡ç†ä¼˜åŒ–
```python
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class CrawlerConfig:
    max_workers: int = 5
    request_delay: tuple = (1, 3)
    timeout: int = 10
    target_sites: List[Dict] = None
    
    def __post_init__(self):
        if self.target_sites is None:
            self.target_sites = []
```

### 10. ç›‘æ§å’Œå‘Šè­¦
```python
import smtplib
from email.mime.text import MIMEText

class AlertManager:
    def send_alert(self, message):
        # å‘é€é‚®ä»¶å‘Šè­¦
        msg = MIMEText(message)
        msg['Subject'] = 'çˆ¬è™«ç³»ç»Ÿå‘Šè­¦'
        
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.send_message(msg)
        server.quit()
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ•°æ®åº“ä¼˜åŒ–**
   - æ·»åŠ ç´¢å¼•: `CREATE INDEX idx_url_hash ON news(url_hash)`
   - ä½¿ç”¨è¿æ¥æ± : `sqlite3.connect(db_path, check_same_thread=False)`

2. **å†…å­˜ç®¡ç†**
   - åˆ†æ‰¹å¤„ç†å¤§é‡æ•°æ®
   - åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡
   - ä½¿ç”¨ç”Ÿæˆå™¨ä»£æ›¿åˆ—è¡¨

3. **ç½‘ç»œä¼˜åŒ–**
   - ä½¿ç”¨HTTP/2
   - å¯ç”¨gzipå‹ç¼©
   - è®¾ç½®åˆç†çš„è¿æ¥æ± å¤§å°

4. **å¹¶å‘æ§åˆ¶**
   - ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
   - å®ç°æ™ºèƒ½é‡è¯•æœºåˆ¶
   - æ·»åŠ ç†”æ–­å™¨æ¨¡å¼

## âœ… ä¿®å¤éªŒè¯

ä¿®å¤åçš„ç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿ:
1. âœ… é«˜çº§çˆ¬è™«æ­£å¸¸å¯åŠ¨ï¼Œä¸å†å‡ºç° `'target_sites'` é”™è¯¯
2. âœ… åŸºç¡€çˆ¬è™«èƒ½å¤ŸæˆåŠŸæå–æ–°é—»é“¾æ¥å’Œæ ‡é¢˜
3. âœ… Webç•Œé¢æ­£å¸¸æ˜¾ç¤ºçˆ¬å–è¿›åº¦å’Œç»“æœ
4. âœ… æ•°æ®èƒ½å¤Ÿæ­£ç¡®ä¿å­˜åˆ°æ•°æ®åº“å’Œæ–‡ä»¶

## ğŸ”„ æŒç»­æ”¹è¿›

å»ºè®®å®šæœŸè¿›è¡Œä»¥ä¸‹ç»´æŠ¤:
1. æ›´æ–°ç½‘ç«™é€‰æ‹©å™¨é€‚é…é¡µé¢å˜åŒ–
2. ç›‘æ§çˆ¬å–æˆåŠŸç‡å’Œæ€§èƒ½æŒ‡æ ‡
3. ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½
4. æ›´æ–°User-Agentå’Œä»£ç†æ± 
5. å¤‡ä»½é‡è¦æ•°æ®å’Œé…ç½®

---

**ä¿®å¤æ—¶é—´**: 2025-06-28  
**ä¿®å¤ç‰ˆæœ¬**: v1.1  
**ä¸‹æ¬¡æ£€æŸ¥**: å»ºè®®1å‘¨åè¿›è¡ŒåŠŸèƒ½éªŒè¯