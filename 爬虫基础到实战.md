# 爬虫基础到实战指南

## 目录

- [1. 爬虫基础知识](#1-爬虫基础知识)
  - [1.1 什么是网络爬虫](#11-什么是网络爬虫)
  - [1.2 爬虫的工作原理](#12-爬虫的工作原理)
  - [1.3 爬虫的合法性与伦理](#13-爬虫的合法性与伦理)
  - [1.4 常见的爬虫类型](#14-常见的爬虫类型)
- [2. 网络基础知识](#2-网络基础知识)
  - [2.1 HTTP/HTTPS协议](#21-httphttps协议)
  - [2.2 URL结构](#22-url结构)
  - [2.3 常见的HTTP状态码](#23-常见的http状态码)
  - [2.4 HTTP请求方法](#24-http请求方法)
  - [2.5 HTTP请求与响应结构](#25-http请求与响应结构)
- [3. Python爬虫基础库](#3-python爬虫基础库)
  - [3.1 Requests库](#31-requests库)
  - [3.2 BeautifulSoup库](#32-beautifulsoup库)
  - [3.3 Lxml库](#33-lxml库)
  - [3.4 Selenium库](#34-selenium库)
  - [3.5 Scrapy框架](#35-scrapy框架)
- [4. 数据解析技术](#4-数据解析技术)
  - [4.1 正则表达式](#41-正则表达式)
  - [4.2 XPath语法](#42-xpath语法)
  - [4.3 CSS选择器](#43-css选择器)
  - [4.4 JSON解析](#44-json解析)
- [5. 爬虫进阶技术](#5-爬虫进阶技术)
  - [5.1 动态页面抓取](#51-动态页面抓取)
  - [5.2 验证码识别](#52-验证码识别)
  - [5.3 模拟登录](#53-模拟登录)
  - [5.4 代理IP的使用](#54-代理ip的使用)
  - [5.5 User-Agent池](#55-user-agent池)
  - [5.6 分布式爬虫](#56-分布式爬虫)
- [6. 反爬虫与反反爬技术](#6-反爬虫与反反爬技术)
  - [6.1 常见的反爬虫机制](#61-常见的反爬虫机制)
  - [6.2 请求频率控制](#62-请求频率控制)
  - [6.3 Cookie与Session处理](#63-cookie与session处理)
  - [6.4 JavaScript混淆的应对](#64-javascript混淆的应对)
  - [6.5 验证码绕过技术](#65-验证码绕过技术)
- [7. 数据存储](#7-数据存储)
  - [7.1 文本文件存储](#71-文本文件存储)
  - [7.2 JSON/CSV/Excel存储](#72-jsoncsv存储)
  - [7.3 关系型数据库存储](#73-关系型数据库存储)
  - [7.4 非关系型数据库存储](#74-非关系型数据库存储)
- [8. 爬虫实战案例](#8-爬虫实战案例)
  - [8.1 静态网页爬取案例](#81-静态网页爬取案例)
  - [8.2 动态网页爬取案例](#82-动态网页爬取案例)
  - [8.3 模拟登录爬取案例](#83-模拟登录爬取案例)
  - [8.4 分布式爬虫案例](#84-分布式爬虫案例)
- [9. 爬虫项目优化](#9-爬虫项目优化)
  - [9.1 性能优化](#91-性能优化)
  - [9.2 代码结构优化](#92-代码结构优化)
  - [9.3 异常处理](#93-异常处理)
  - [9.4 日志记录](#94-日志记录)
- [10. 爬虫的商业应用](#10-爬虫的商业应用)
  - [10.1 数据采集与分析](#101-数据采集与分析)
  - [10.2 搜索引擎](#102-搜索引擎)
  - [10.3 舆情监控](#103-舆情监控)
  - [10.4 价格比较](#104-价格比较)

## 1. 爬虫基础知识

### 1.1 什么是网络爬虫

网络爬虫（Web Crawler）是一种按照一定的规则，自动地抓取互联网信息的程序或脚本。它模拟人类浏览网页的行为，访问网站并获取所需的数据。

爬虫在现代互联网中扮演着重要角色，被广泛应用于搜索引擎、数据挖掘、市场分析、学术研究等领域。

### 1.2 爬虫的工作原理

爬虫的基本工作流程包括以下几个步骤：

1. **URL管理**：维护待爬取的URL队列和已爬取URL集合
2. **网页下载**：通过HTTP/HTTPS协议获取网页内容
3. **网页解析**：从网页中提取有用信息和新的URL
4. **数据存储**：将提取的信息保存到文件或数据库中
5. **任务调度**：控制爬取的频率、顺序和深度

这个过程可以表示为以下流程图：

```
开始 → URL管理 → 网页下载 → 网页解析 → 数据存储 → 结束
        ↑                 |
        |_________________|
         (提取新的URL)
```

### 1.3 爬虫的合法性与伦理

在开发和使用爬虫时，需要注意以下法律和伦理问题：

1. **robots.txt协议**：尊重网站的robots.txt文件，它规定了爬虫可以访问网站的哪些部分
2. **版权问题**：抓取的内容可能受版权保护，未经授权不得用于商业用途
3. **服务条款**：许多网站的服务条款明确禁止爬虫行为
4. **访问频率**：过高的访问频率可能对目标网站造成负担，甚至构成拒绝服务攻击
5. **个人隐私**：避免抓取和存储个人隐私信息

### 1.4 常见的爬虫类型

根据不同的应用场景和技术特点，爬虫可以分为以下几类：

1. **通用爬虫**：如搜索引擎爬虫，抓取互联网上的所有可访问页面
2. **聚焦爬虫**：针对特定主题或特定类型的网页进行定向抓取
3. **增量式爬虫**：只抓取新产生或更新的网页
4. **深层爬虫**：能够抓取深层网页，如需要登录、表单提交才能访问的页面
5. **分布式爬虫**：利用多台计算机协同工作，提高爬取效率

## 2. 网络基础知识

### 2.1 HTTP/HTTPS协议

HTTP（超文本传输协议）是网络爬虫与网站服务器通信的基础协议。

**HTTP特点**：

- 无状态：每次请求相互独立
- 基于请求-响应模式
- 默认端口80

**HTTPS特点**：

- HTTP + SSL/TLS加密
- 提供数据加密和身份验证
- 默认端口443

### 2.2 URL结构

URL（统一资源定位符）是互联网上资源的地址，其基本结构为：

```
协议://主机名[:端口]/路径[?查询参数][#片段标识符]
```

例如：`https://www.example.com:443/path/to/resource?id=123&sort=asc#section2`

- 协议：https
- 主机名：www.example.com
- 端口：443（HTTPS默认端口，通常省略）
- 路径：/path/to/resource
- 查询参数：id=123&sort=asc
- 片段标识符：section2

### 2.3 常见的HTTP状态码

HTTP状态码指示特定HTTP请求是否已成功完成。常见的状态码包括：

- **1xx**：信息性响应
  
  - 100 Continue：继续请求

- **2xx**：成功响应
  
  - 200 OK：请求成功
  - 201 Created：已创建
  - 204 No Content：成功但无内容返回

- **3xx**：重定向
  
  - 301 Moved Permanently：永久重定向
  - 302 Found：临时重定向
  - 304 Not Modified：资源未修改

- **4xx**：客户端错误
  
  - 400 Bad Request：请求语法错误
  - 401 Unauthorized：需要身份验证
  - 403 Forbidden：服务器拒绝请求
  - 404 Not Found：资源不存在
  - 429 Too Many Requests：请求过多（常用于限制爬虫）

- **5xx**：服务器错误
  
  - 500 Internal Server Error：服务器内部错误
  - 502 Bad Gateway：网关错误
  - 503 Service Unavailable：服务不可用

### 2.4 HTTP请求方法

HTTP定义了多种请求方法，爬虫最常用的是：

- **GET**：请求指定资源，只应用于获取数据
- **POST**：向指定资源提交数据，可能导致新资源的创建或已有资源的修改
- **HEAD**：与GET相同，但只返回HTTP头，不返回文档主体
- **PUT**：上传指定的URI表示
- **DELETE**：删除指定资源
- **OPTIONS**：返回服务器支持的HTTP方法

### 2.5 HTTP请求与响应结构

**HTTP请求结构**：

```
请求行：方法 URL HTTP版本
请求头：键值对形式的元数据
空行
请求体：POST等方法的数据
```

示例：

```
GET /index.html HTTP/1.1
Host: www.example.com
User-Agent: Mozilla/5.0
Accept: text/html
```

**HTTP响应结构**：

```
状态行：HTTP版本 状态码 状态描述
响应头：键值对形式的元数据
空行
响应体：返回的内容
```

示例：

```
HTTP/1.1 200 OK
Content-Type: text/html
Content-Length: 1234

<!DOCTYPE html>
<html>...
```

## 3. Python爬虫基础库

### 3.1 Requests库

Requests是Python最流行的HTTP客户端库，提供了简单易用的API来发送HTTP请求。

**安装**：

```bash
pip install requests
```

**基本用法**：

```python
import requests

# 发送GET请求
response = requests.get('https://www.example.com')

# 查看响应状态码
print(response.status_code)  # 200

# 查看响应内容
print(response.text)

# 发送带参数的GET请求
params = {'key1': 'value1', 'key2': 'value2'}
response = requests.get('https://www.example.com/search', params=params)

# 发送POST请求
data = {'username': 'user', 'password': 'pass'}
response = requests.post('https://www.example.com/login', data=data)

# 设置请求头
headers = {'User-Agent': 'Mozilla/5.0'}
response = requests.get('https://www.example.com', headers=headers)

# 处理Cookies
response = requests.get('https://www.example.com', cookies={'session_id': '123'})

# 会话对象，保持Cookies
session = requests.Session()
session.get('https://www.example.com')
response = session.get('https://www.example.com/profile')  # 会自动带上之前的Cookies

# 处理超时
response = requests.get('https://www.example.com', timeout=5)

# 处理重定向
response = requests.get('https://www.example.com', allow_redirects=True)
```

### 3.2 BeautifulSoup库

BeautifulSoup是一个用于解析HTML和XML文档的库，它可以从文档中提取数据，修改文档结构。

**安装**：

```bash
pip install beautifulsoup4
```

**基本用法**：

```python
from bs4 import BeautifulSoup
import requests

# 获取网页内容
response = requests.get('https://www.example.com')
html = response.text

# 创建BeautifulSoup对象
soup = BeautifulSoup(html, 'html.parser')  # 也可以使用'lxml'作为解析器

# 查找第一个符合条件的标签
title = soup.title  # 获取<title>标签
print(title.text)  # 获取标签文本内容

first_paragraph = soup.p  # 获取第一个<p>标签
print(first_paragraph['class'])  # 获取标签的class属性

# 查找所有符合条件的标签
all_links = soup.find_all('a')  # 获取所有<a>标签
for link in all_links:
    print(link.get('href'))  # 获取链接地址

# 使用CSS选择器
results = soup.select('div.container p.content')  # 选择class为content的p标签，且其父元素是class为container的div
for result in results:
    print(result.text)

# 使用属性查找
login_form = soup.find('form', {'id': 'login'})
username_field = login_form.find('input', {'name': 'username'})

# 获取标签的所有属性
print(login_form.attrs)

# 导航文档树
parent = first_paragraph.parent  # 父节点
next_sibling = first_paragraph.next_sibling  # 下一个兄弟节点
previous_sibling = first_paragraph.previous_sibling  # 上一个兄弟节点
children = list(first_paragraph.children)  # 子节点
```

### 3.3 Lxml库

Lxml是一个高性能的XML和HTML解析库，它基于libxml2和libxslt库，支持XPath语法。

**安装**：

```bash
pip install lxml
```

**基本用法**：

```python
from lxml import etree
import requests

# 获取网页内容
response = requests.get('https://www.example.com')
html = response.text

# 创建解析对象
selector = etree.HTML(html)

# 使用XPath选择元素
title = selector.xpath('//title/text()')  # 获取标题文本
print(title[0] if title else None)

# 获取所有链接
links = selector.xpath('//a/@href')  # 获取所有a标签的href属性
for link in links:
    print(link)

# 获取特定元素
divs = selector.xpath('//div[@class="content"]')  # 获取class为content的div
for div in divs:
    # 在当前div中继续查找
    paragraphs = div.xpath('./p/text()')  # 注意这里的点表示当前节点
    for p in paragraphs:
        print(p)

# 获取元素属性
img_srcs = selector.xpath('//img/@src')  # 获取所有图片的src属性

# 条件选择
specific_links = selector.xpath('//a[contains(@href, "example")]/@href')  # 获取href包含example的链接

# 获取元素的文本内容
text_content = selector.xpath('string(//div[@id="main"])')  # 获取id为main的div的所有文本内容，包括子元素
```

### 3.4 Selenium库

Selenium是一个用于自动化浏览器操作的工具，可以模拟用户操作，处理JavaScript渲染的页面。

**安装**：

```bash
pip install selenium
```

还需要安装对应浏览器的驱动程序，如ChromeDriver、GeckoDriver等。

**基本用法**：

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# 创建浏览器实例
driver = webdriver.Chrome()  # 需要安装ChromeDriver

# 访问网页
driver.get('https://www.example.com')

# 等待页面加载完成
time.sleep(2)  # 简单等待

# 更好的等待方式
try:
    element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, 'myElement'))
    )
finally:
    pass

# 查找元素
element = driver.find_element(By.ID, 'search')  # 通过ID查找
elements = driver.find_elements(By.CLASS_NAME, 'result')  # 通过类名查找多个元素
link = driver.find_element(By.XPATH, '//a[@href="https://www.example.com"]')  # 通过XPath查找

# 操作元素
element.send_keys('search term')  # 输入文本
element.send_keys(Keys.RETURN)  # 按回车键
link.click()  # 点击链接

# 获取元素信息
text = element.text  # 获取元素文本
attribute = element.get_attribute('href')  # 获取元素属性

# 执行JavaScript
driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')  # 滚动到页面底部

# 处理弹窗
alert = driver.switch_to.alert
alert.accept()  # 点击确定
alert.dismiss()  # 点击取消

# 处理iframe
driver.switch_to.frame('iframe_id')  # 切换到iframe
driver.switch_to.default_content()  # 切回主文档

# 处理窗口
driver.switch_to.window(driver.window_handles[1])  # 切换到第二个窗口

# 获取页面源码
html = driver.page_source

# 截图
driver.save_screenshot('screenshot.png')

# 关闭浏览器
driver.quit()
```

### 3.5 Scrapy框架

Scrapy是一个功能强大的爬虫框架，提供了一整套工具，用于高效地抓取网站和提取结构化数据。

**安装**：

```bash
pip install scrapy
```

**创建项目**：

```bash
scrapy startproject myproject
cd myproject
scrapy genspider example example.com
```

**基本结构**：

```
myproject/
    scrapy.cfg            # 项目配置文件
    myproject/            # 项目Python模块
        __init__.py
        items.py          # 项目items定义
        middlewares.py    # 项目中间件
        pipelines.py      # 项目管道
        settings.py       # 项目设置
        spiders/          # 爬虫目录
            __init__.py
            example.py    # 示例爬虫
```

**示例爬虫**：

```python
# myproject/spiders/example.py
import scrapy

class ExampleSpider(scrapy.Spider):
    name = 'example'
    allowed_domains = ['example.com']
    start_urls = ['http://example.com/']

    def parse(self, response):
        # 提取数据
        title = response.css('title::text').get()
        yield {'title': title}

        # 提取链接并跟踪
        for href in response.css('a::attr(href)'):
            yield response.follow(href, self.parse)
```

**定义Item**：

```python
# myproject/items.py
import scrapy

class MyItem(scrapy.Item):
    title = scrapy.Field()
    url = scrapy.Field()
    content = scrapy.Field()
```

**使用Item**：

```python
# myproject/spiders/example.py
from myproject.items import MyItem

class ExampleSpider(scrapy.Spider):
    # ...

    def parse(self, response):
        item = MyItem()
        item['title'] = response.css('title::text').get()
        item['url'] = response.url
        item['content'] = response.css('div.content::text').get()
        yield item
```

**定义Pipeline**：

```python
# myproject/pipelines.py
class MyPipeline:
    def process_item(self, item, spider):
        # 处理item，如清洗数据、存储到数据库等
        return item
```

**启用Pipeline**：

```python
# myproject/settings.py
ITEM_PIPELINES = {
    'myproject.pipelines.MyPipeline': 300,  # 数字表示优先级，越小越先执行
}
```

**运行爬虫**：

```bash
scrapy crawl example
```

**保存结果**：

```bash
scrapy crawl example -o results.json  # 保存为JSON
scrapy crawl example -o results.csv   # 保存为CSV
scrapy crawl example -o results.xml   # 保存为XML
```

## 4. 数据解析技术

### 4.1 正则表达式

正则表达式是一种强大的文本模式匹配工具，在爬虫中常用于从非结构化文本中提取数据。

**Python中的正则表达式**：

```python
import re

# 匹配单个结果
text = 'The price is $25.99'
result = re.search(r'\$([0-9]+\.[0-9]+)', text)
if result:
    price = result.group(1)  # 提取第一个捕获组
    print(price)  # 25.99

# 匹配多个结果
text = 'Prices: $25.99, $10.50, $5.75'
results = re.findall(r'\$([0-9]+\.[0-9]+)', text)
print(results)  # ['25.99', '10.50', '5.75']

# 替换
text = 'Contact us at info@example.com or support@example.com'
new_text = re.sub(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', r'[EMAIL PROTECTED]', text)
print(new_text)  # Contact us at [EMAIL PROTECTED] or [EMAIL PROTECTED]

# 分割
text = 'apple,banana,orange;grape|melon'
fruits = re.split(r'[,;|]', text)
print(fruits)  # ['apple', 'banana', 'orange', 'grape', 'melon']
```

**常用正则表达式模式**：

- 匹配邮箱：`[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}`
- 匹配URL：`https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+(?:/[^\s]*)?`
- 匹配IP地址：`\b(?:\d{1,3}\.){3}\d{1,3}\b`
- 匹配HTML标签：`<([a-z][a-z0-9]*)[^>]*>(.*?)</\1>`

### 4.2 XPath语法

XPath是一种在XML文档中查找信息的语言，也适用于HTML文档。

**基本语法**：

- `/`：从根节点选取
- `//`：从当前节点选取，不考虑位置
- `.`：选取当前节点
- `..`：选取当前节点的父节点
- `@`：选取属性

**示例**：

- `//div`：选择所有div元素
- `//div[@class="content"]`：选择class为content的div元素
- `//div/p`：选择所有div元素的直接子p元素
- `//div//p`：选择所有div元素下的所有p元素（不限于直接子元素）
- `//a[@href]`：选择所有有href属性的a元素
- `//a[@href="https://example.com"]`：选择href为https://example.com的a元素
- `//div[contains(@class, "main")]`：选择class属性包含main的div元素
- `//div[position() <= 3]`：选择前三个div元素
- `//div/text()`：选择div元素的文本内容
- `//div/p[1]`：选择每个div的第一个p子元素
- `//div | //p`：选择所有div和p元素

### 4.3 CSS选择器

CSS选择器是一种从HTML文档中选择元素的模式，语法简洁，易于理解。

**基本语法**：

- 元素选择器：`p`（选择所有p元素）
- ID选择器：`#id`（选择id为id的元素）
- 类选择器：`.class`（选择class为class的元素）
- 属性选择器：`[attr]`（选择有attr属性的元素）
- 后代选择器：`div p`（选择div内的所有p元素）
- 子元素选择器：`div > p`（选择div的直接子p元素）
- 相邻兄弟选择器：`div + p`（选择紧接在div后的p元素）
- 通用兄弟选择器：`div ~ p`（选择div后的所有p兄弟元素）

**示例**：

- `div.content`：选择class为content的div元素
- `#main > p`：选择id为main的元素的直接子p元素
- `a[href]`：选择有href属性的a元素
- `a[href="https://example.com"]`：选择href为https://example.com的a元素
- `a[href*="example"]`：选择href包含example的a元素
- `a[href^="https"]`：选择href以https开头的a元素
- `a[href$=".pdf"]`：选择href以.pdf结尾的a元素
- `div p:first-child`：选择div中的第一个子p元素
- `div p:nth-child(2)`：选择div中的第二个子p元素

### 4.4 JSON解析

许多现代网站使用JSON格式传输数据，特别是在AJAX请求中。Python提供了内置的json模块来处理JSON数据。

**基本用法**：

```python
import json
import requests

# 解析JSON字符串
json_str = '{"name": "John", "age": 30, "city": "New York"}'
data = json.loads(json_str)
print(data['name'])  # John

# 从API获取JSON数据
response = requests.get('https://api.example.com/data')
data = response.json()  # 自动解析JSON响应

# 处理嵌套JSON
if 'results' in data and isinstance(data['results'], list):
    for item in data['results']:
        print(item['title'])

# 将Python对象转换为JSON字符串
python_obj = {'name': 'Alice', 'age': 25, 'skills': ['Python', 'JavaScript']}
json_str = json.dumps(python_obj, indent=4)  # 美化输出
print(json_str)

# 将JSON写入文件
with open('data.json', 'w') as f:
    json.dump(python_obj, f, indent=4)

# 从文件读取JSON
with open('data.json', 'r') as f:
    data = json.load(f)
```

## 5. 爬虫进阶技术

### 5.1 动态页面抓取

现代网站大量使用JavaScript动态加载内容，这给传统爬虫带来了挑战。处理动态页面的方法有：

1. **使用Selenium等浏览器自动化工具**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver = webdriver.Chrome()
driver.get('https://example.com')

# 等待动态内容加载
WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CLASS_NAME, 'dynamic-content'))
)

# 获取动态加载的内容
content = driver.find_element(By.CLASS_NAME, 'dynamic-content').text

driver.quit()
```

2. **分析网络请求，直接请求API**

```python
import requests

# 通过分析浏览器网络请求，找到数据API
api_url = 'https://api.example.com/data?page=1'
headers = {
    'User-Agent': 'Mozilla/5.0',
    'Referer': 'https://example.com',
    'X-Requested-With': 'XMLHttpRequest'
}

response = requests.get(api_url, headers=headers)
data = response.json()

# 处理API返回的数据
for item in data['items']:
    print(item['title'])
```

3. **使用专门的JavaScript渲染服务**

```python
import requests

# 使用Splash服务
splash_url = 'http://localhost:8050/render.html'
params = {
    'url': 'https://example.com',
    'wait': 2,  # 等待JavaScript执行的秒数
}

response = requests.get(splash_url, params=params)
rendered_html = response.text

# 然后用BeautifulSoup或lxml解析渲染后的HTML
```

### 5.2 验证码识别

验证码是网站防止自动化访问的常用手段。处理验证码的方法有：

1. **使用OCR库识别简单验证码**

```python
from PIL import Image
import pytesseract
import requests
from io import BytesIO

# 下载验证码图片
response = requests.get('https://example.com/captcha.php')
image = Image.open(BytesIO(response.content))

# 预处理图片（可选）
image = image.convert('L')  # 转为灰度图
threshold = 127
image = image.point(lambda p: p > threshold and 255)

# 识别验证码
captcha_text = pytesseract.image_to_string(image)
print(captcha_text)
```

2. **使用专业的验证码识别服务**

```python
import requests

# 下载验证码图片
captcha_url = 'https://example.com/captcha.php'
response = requests.get(captcha_url)

# 使用第三方验证码识别服务
api_url = 'https://api.captcha-service.com/recognize'
files = {'image': response.content}
api_response = requests.post(api_url, files=files, data={'api_key': 'YOUR_API_KEY'})

captcha_text = api_response.json()['text']
print(captcha_text)
```

3. **对于复杂验证码，可能需要使用深度学习模型**

```python
# 使用预训练的深度学习模型
from keras.models import load_model
from PIL import Image
import numpy as np
from io import BytesIO
import requests

# 加载模型
model = load_model('captcha_model.h5')

# 下载验证码
response = requests.get('https://example.com/captcha.php')
image = Image.open(BytesIO(response.content))

# 预处理图片
image = image.resize((100, 30))  # 调整大小
image = np.array(image) / 255.0  # 归一化
image = np.expand_dims(image, axis=0)  # 添加批次维度

# 预测
prediction = model.predict(image)
# 解码预测结果...
```

### 5.3 模拟登录

许多网站需要登录才能访问内容。模拟登录的方法有：

1. **使用表单提交**

```python
import requests

# 创建会话对象，保持Cookies
session = requests.Session()

# 获取登录页面，可能包含一些必要的token
login_page = session.get('https://example.com/login')

# 提交登录表单
login_data = {
    'username': 'your_username',
    'password': 'your_password',
    'csrf_token': 'extract_from_login_page',  # 从登录页面提取
}

login_response = session.post('https://example.com/login', data=login_data)

# 检查是否登录成功
if 'Welcome' in login_response.text or login_response.url.endswith('/dashboard'):
    print('Login successful')

    # 访问需要登录的页面
    protected_page = session.get('https://example.com/protected')
    print(protected_page.text)
else:
    print('Login failed')
```

2. **使用Selenium模拟用户操作**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
import time

driver = webdriver.Chrome()
driver.get('https://example.com/login')

# 填写登录表单
driver.find_element(By.ID, 'username').send_keys('your_username')
driver.find_element(By.ID, 'password').send_keys('your_password')
driver.find_element(By.ID, 'login-button').click()

# 等待登录完成
time.sleep(2)

# 检查是否登录成功
if 'Welcome' in driver.page_source or '/dashboard' in driver.current_url:
    print('Login successful')

    # 访问需要登录的页面
    driver.get('https://example.com/protected')
    print(driver.page_source)
else:
    print('Login failed')

driver.quit()
```

3. **使用API登录**

```python
import requests
import json

# 通过分析网络请求，找到登录API
api_url = 'https://api.example.com/login'
headers = {
    'User-Agent': 'Mozilla/5.0',
    'Content-Type': 'application/json',
    'X-Requested-With': 'XMLHttpRequest'
}

login_data = {
    'username': 'your_username',
    'password': 'your_password'
}

response = requests.post(api_url, headers=headers, data=json.dumps(login_data))
result = response.json()

if result.get('success') or result.get('token'):
    print('Login successful without captcha')
```

3. **使用无头浏览器自动处理**

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time

# 配置无头浏览器
options = Options()
options.add_argument('--headless')

driver = webdriver.Chrome(options=options)
driver.get('https://example.com/login')

# 填写登录信息
driver.find_element(By.ID, 'username').send_keys('your_username')
driver.find_element(By.ID, 'password').send_keys('your_password')

# 如果有验证码，可以尝试自动识别或手动处理
# ...

# 提交表单
driver.find_element(By.ID, 'login-button').click()

# 等待登录完成
time.sleep(2)

# 检查是否登录成功
if 'Welcome' in driver.page_source:
    print('Login successful')

    # 获取Cookie
    cookies = driver.get_cookies()

    # 转换为requests可用的格式
    cookies_dict = {}
    for cookie in cookies:
        cookies_dict[cookie['name']] = cookie['value']

    # 后续可以使用requests+cookies访问
    session = requests.Session()
    session.cookies.update(cookies_dict)

    # 访问需要登录的页面
    response = session.get('https://example.com/protected')
    print(response.text)

driver.quit()
```

## 7. 数据存储

### 7.1 文本文件存储

最简单的数据存储方式是保存为文本文件。

```python
# 保存为TXT文件
def save_to_txt(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(str(item) + '\n')

# 使用示例
data = ['Item 1', 'Item 2', 'Item 3']
save_to_txt(data, 'output.txt')
```

### 7.2 JSON/CSV存储

JSON和CSV是常用的结构化数据存储格式。

1. **JSON存储**

```python
import json

# 保存为JSON文件
def save_to_json(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

# 从JSON文件加载
def load_from_json(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return json.load(f)

# 使用示例
data = [
    {'name': 'Product 1', 'price': 99.9, 'available': True},
    {'name': 'Product 2', 'price': 149.9, 'available': False},
    {'name': 'Product 3', 'price': 199.9, 'available': True}
]

save_to_json(data, 'products.json')
loaded_data = load_from_json('products.json')
```

2. **CSV存储**

```python
import csv

# 保存为CSV文件
def save_to_csv(data, filename, headers=None):
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)

        # 写入表头
        if headers:
            writer.writerow(headers)

        # 写入数据
        for row in data:
            writer.writerow(row)

# 从CSV文件加载
def load_from_csv(filename, has_headers=True):
    with open(filename, 'r', encoding='utf-8', newline='') as f:
        reader = csv.reader(f)

        if has_headers:
            headers = next(reader)  # 读取表头
            data = [row for row in reader]  # 读取数据
            return headers, data
        else:
            return [row for row in reader]

# 使用示例
headers = ['Name', 'Price', 'Available']
data = [
    ['Product 1', 99.9, 'Yes'],
    ['Product 2', 149.9, 'No'],
    ['Product 3', 199.9, 'Yes']
]

save_to_csv(data, 'products.csv', headers)
headers, loaded_data = load_from_csv('products.csv')
```

3. **Excel存储**

```python
import pandas as pd

# 保存为Excel文件
def save_to_excel(data, filename, sheet_name='Sheet1'):
    df = pd.DataFrame(data)
    df.to_excel(filename, sheet_name=sheet_name, index=False)

# 从Excel文件加载
def load_from_excel(filename, sheet_name='Sheet1'):
    return pd.read_excel(filename, sheet_name=sheet_name)

# 使用示例
data = {
    'Name': ['Product 1', 'Product 2', 'Product 3'],
    'Price': [99.9, 149.9, 199.9],
    'Available': [True, False, True]
}

save_to_excel(data, 'products.xlsx')
df = load_from_excel('products.xlsx')
print(df)
```

### 7.3 关系型数据库存储

关系型数据库适合存储结构化数据，常用的有MySQL、PostgreSQL、SQLite等。

1. **SQLite示例**

```python
import sqlite3

# 连接数据库（如果不存在则创建）
conn = sqlite3.connect('data.db')
cursor = conn.cursor()

# 创建表
cursor.execute('''
CREATE TABLE IF NOT EXISTS products (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    price REAL,
    available INTEGER
)
''')

# 插入数据
def insert_product(name, price, available):
    cursor.execute('INSERT INTO products (name, price, available) VALUES (?, ?, ?)',
                  (name, price, 1 if available else 0))
    conn.commit()

# 查询数据
def get_all_products():
    cursor.execute('SELECT * FROM products')
    return cursor.fetchall()

def get_product_by_id(product_id):
    cursor.execute('SELECT * FROM products WHERE id = ?', (product_id,))
    return cursor.fetchone()

# 使用示例
insert_product('Product 1', 99.9, True)
insert_product('Product 2', 149.9, False)
insert_product('Product 3', 199.9, True)

all_products = get_all_products()
for product in all_products:
    print(product)

# 关闭连接
conn.close()
```

2. **MySQL示例**

```python
import mysql.connector

# 连接数据库
conn = mysql.connector.connect(
    host='localhost',
    user='username',
    password='password',
    database='mydatabase'
)
cursor = conn.cursor()

# 创建表
cursor.execute('''
CREATE TABLE IF NOT EXISTS products (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    price DECIMAL(10, 2),
    available BOOLEAN
)
''')

# 插入数据
def insert_product(name, price, available):
    cursor.execute('INSERT INTO products (name, price, available) VALUES (%s, %s, %s)',
                  (name, price, available))
    conn.commit()

# 查询数据
def get_all_products():
    cursor.execute('SELECT * FROM products')
    return cursor.fetchall()

def get_product_by_id(product_id):
    cursor.execute('SELECT * FROM products WHERE id = %s', (product_id,))
    return cursor.fetchone()

# 使用示例
insert_product('Product 1', 99.9, True)
insert_product('Product 2', 149.9, False)
insert_product('Product 3', 199.9, True)

all_products = get_all_products()
for product in all_products:
    print(product)

# 关闭连接
cursor.close()
conn.close()
```

### 7.4 非关系型数据库存储

非关系型数据库适合存储非结构化或半结构化数据，常用的有MongoDB、Redis等。

1. **MongoDB示例**

```python
from pymongo import MongoClient

# 连接MongoDB
client = MongoClient('mongodb://localhost:27017/')
db = client['crawler_db']  # 数据库名
collection = db['products']  # 集合名

# 插入数据
def insert_product(product):
    return collection.insert_one(product).inserted_id

# 插入多条数据
def insert_many_products(products):
    return collection.insert_many(products).inserted_ids

# 查询数据
def get_all_products():
    return list(collection.find())

def get_product_by_name(name):
    return collection.find_one({'name': name})

# 更新数据
def update_product(name, new_data):
    return collection.update_one(
        {'name': name},  # 查询条件
        {'$set': new_data}  # 更新操作
    ).modified_count

# 删除数据
def delete_product(name):
    return collection.delete_one({'name': name}).deleted_count

# 使用示例
products = [
    {'name': 'Product 1', 'price': 99.9, 'available': True, 'tags': ['electronics', 'gadget']},
    {'name': 'Product 2', 'price': 149.9, 'available': False, 'tags': ['electronics']},
    {'name': 'Product 3', 'price': 199.9, 'available': True, 'tags': ['electronics', 'premium']}
]

# 插入多条数据
insert_many_products(products)

# 查询
all_products = get_all_products()
for product in all_products:
    print(product)

# 更新
update_product('Product 1', {'price': 89.9, 'on_sale': True})

# 删除
delete_product('Product 2')

# 关闭连接
client.close()
```

2. **Redis示例**

```python
import redis
import json

# 连接Redis
r = redis.Redis(host='localhost', port=6379, db=0)

# 存储字符串
def set_string(key, value):
    return r.set(key, value)

# 存储哈希表
def set_hash(key, mapping):
    return r.hset(key, mapping=mapping)

# 存储列表
def push_to_list(key, *values):
    return r.rpush(key, *values)

# 存储集合
def add_to_set(key, *values):
    return r.sadd(key, *values)

# 存储有序集合
def add_to_zset(key, mapping):
    return r.zadd(key, mapping)

# 存储JSON对象
def set_json(key, obj):
    return r.set(key, json.dumps(obj))

# 获取JSON对象
def get_json(key):
    value = r.get(key)
    if value:
        return json.loads(value)
    return None

# 使用示例
# 存储产品信息
product = {
    'name': 'Product 1',
    'price': 99.9,
    'available': True,
    'tags': ['electronics', 'gadget']
}

# 存储为JSON
set_json('product:1', product)

# 存储为哈希表
product_hash = {
    'name': 'Product 1',
    'price': '99.9',
    'available': '1'
}
set_hash('product:1:hash', product_hash)

# 存储标签到集合
add_to_set('product:1:tags', 'electronics', 'gadget')

# 获取数据
retrieved_product = get_json('product:1')
print(retrieved_product)

# 关闭连接
r.close()
```

## 8. 爬虫实战案例

### 8.1 静态网页爬取案例

以下是一个爬取静态新闻网站的示例：

```python
import requests
from bs4 import BeautifulSoup
import csv
import time
import random

# 定义请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# 爬取新闻列表页
def crawl_news_list(url):
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 找到所有新闻项
    news_items = soup.select('div.news-item')

    news_links = []
    for item in news_items:
        # 提取标题和链接
        title_element = item.select_one('h2.title a')
        if title_element:
            title = title_element.text.strip()
            link = title_element['href']

            # 如果链接是相对路径，转为绝对路径
            if link.startswith('/'):
                link = 'https://example.com' + link

            news_links.append({
                'title': title,
                'link': link
            })

    return news_links

# 爬取新闻详情页
def crawl_news_detail(url):
    # 添加随机延时
    time.sleep(random.uniform(1, 3))

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 提取新闻内容
    title = soup.select_one('h1.article-title').text.strip() if soup.select_one('h1.article-title') else ''

    # 提取发布时间
    publish_time = soup.select_one('span.publish-time').text.strip() if soup.select_one('span.publish-time') else ''

    # 提取作者
    author = soup.select_one('span.author').text.strip() if soup.select_one('span.author') else ''

    # 提取正文
    content_elements = soup.select('div.article-content p')
    content = '\n'.join([p.text.strip() for p in content_elements])

    return {
        'title': title,
        'publish_time': publish_time,
        'author': author,
        'content': content,
        'url': url
    }

# 保存为CSV文件
def save_to_csv(data, filename):
    with open(filename, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=['title', 'publish_time', 'author', 'content', 'url'])
        writer.writeheader()
        writer.writerows(data)

# 主函数
def main():
    base_url = 'https://example.com/news'

    # 爬取多个页面
    all_news = []
    for page in range(1, 6):  # 爬取5页
        page_url = f'{base_url}?page={page}'
        print(f'Crawling page {page}: {page_url}')

        news_links = crawl_news_list(page_url)

        # 爬取每条新闻的详情
        for news in news_links:
            print(f'Crawling news: {news["title"]}')
            detail = crawl_news_detail(news['link'])
            all_news.append(detail)

        # 页面间延时
        time.sleep(random.uniform(3, 5))

    # 保存结果
    save_to_csv(all_news, 'news_data.csv')
    print(f'Total {len(all_news)} news articles saved to news_data.csv')

if __name__ == '__main__':
    main()
```

### 8.2 动态网页爬取案例

以下是一个爬取使用AJAX加载数据的电商网站的示例：

```python
import requests
import json
import time
import random
import pandas as pd

# 定义请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Referer': 'https://example.com/products',
    'X-Requested-With': 'XMLHttpRequest'
}

# 爬取商品列表
def crawl_product_list(category_id, page=1, page_size=20):
    # 构造API URL
    api_url = f'https://api.example.com/products'

    # 请求参数
    params = {
        'category_id': category_id,
        'page': page,
        'page_size': page_size,
        'sort': 'popular',
        '_': int(time.time() * 1000)  # 添加时间戳防止缓存
    }

    response = requests.get(api_url, headers=headers, params=params)
    data = response.json()

    # 提取商品列表
    products = data.get('data', {}).get('products', [])
    total_pages = data.get('data', {}).get('total_pages', 1)

    return products, total_pages

# 爬取商品详情
def crawl_product_detail(product_id):
    # 添加随机延时
    time.sleep(random.uniform(1, 2))

    # 构造API URL
    api_url = f'https://api.example.com/product/{product_id}'

    response = requests.get(api_url, headers=headers)
    data = response.json()

    # 提取商品详情
    product = data.get('data', {})

    return product

# 主函数
def main():
    # 要爬取的分类ID
    category_id = 123

    all_products = []
    page = 1

    # 获取第一页，同时获取总页数
    products, total_pages = crawl_product_list(category_id, page)
    all_products.extend(products)

    # 爬取剩余页面
    for page in range(2, total_pages + 1):
        print(f'Crawling page {page}/{total_pages}')
        products, _ = crawl_product_list(category_id, page)
        all_products.extend(products)

        # 页面间延时
        time.sleep(random.uniform(2, 4))

    # 爬取每个商品的详细信息
    detailed_products = []
    for i, product in enumerate(all_products):
        print(f'Crawling product {i+1}/{len(all_products)}: {product["name"]}')
        detail = crawl_product_detail(product['id'])
        detailed_products.append(detail)

    # 转换为DataFrame并保存
    df = pd.DataFrame(detailed_products)
    df.to_excel('products_data.xlsx', index=False)
    print(f'Total {len(detailed_products)} products saved to products_data.xlsx')

if __name__ == '__main__':
    main()
```

### 8.3 模拟登录爬取案例

以下是一个需要登录才能访问的网站爬取示例：

```python
import requests
from bs4 import BeautifulSoup
import time
import json
import os

# 定义请求头
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

# 创建会话对象
session = requests.Session()

# 保存和加载Cookie
def save_cookies(cookies, filename):
    with open(filename, 'w') as f:
        json.dump(cookies, f)

def load_cookies(filename):
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            return json.load(f)
    return None

# 登录函数
def login(username, password):
    # 首先尝试加载已保存的Cookie
    cookies = load_cookies('cookies.json')
    if cookies:
        print('Loading cookies from file')
        session.cookies.update(cookies)

        # 验证Cookie是否有效
        if check_login_status():
            print('Login successful with saved cookies')
            return True
        else:
            print('Saved cookies expired, need to login again')

    # 访问登录页面获取必要的表单参数
    login_url = 'https://example.com/login'
    response = session.get(login_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 提取CSRF令牌（根据实际网站调整选择器）
    csrf_token = soup.select_one('input[name="csrf_token"]')['value']

    # 构造登录数据
    login_data = {
        'username': username,
        'password': password,
        'csrf_token': csrf_token,
        'remember_me': 'true'
    }

    # 发送登录请求
    response = session.post(login_url, data=login_data, headers=headers, allow_redirects=True)

    # 检查登录是否成功
    if check_login_status():
        print('Login successful')
        # 保存Cookie以便下次使用
        save_cookies(session.cookies.get_dict(), 'cookies.json')
        return True
    else:
        print('Login failed')
        return False

# 检查登录状态
def check_login_status():
    profile_url = 'https://example.com/profile'
    response = session.get(profile_url, headers=headers)

    # 根据响应判断是否已登录（例如检查页面是否包含用户名或登录后才有的元素）
    return 'Welcome back' in response.text or response.url == profile_url

# 爬取需要登录才能访问的页面
def crawl_protected_page(url):
    response = session.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 提取页面数据
    # 这里根据实际网站结构进行调整
    title = soup.select_one('h1.page-title').text.strip() if soup.select_one('h1.page-title') else ''
    content = soup.select_one('div.content').text.strip() if soup.select_one('div.content') else ''

    return {
        'title': title,
        'content': content,
        'url': url
    }

# 主函数
def main():
    username = 'your_username'
    password = 'your_password'

    # 登录
    if not login(username, password):
        print('Cannot proceed without login')
        return

    # 要爬取的受保护页面列表
    protected_urls = [
        'https://example.com/protected/page1',
        'https://example.com/protected/page2',
        'https://example.com/protected/page3'
    ]

    # 爬取每个页面
    results = []
    for url in protected_urls:
        print(f'Crawling {url}')
        data = crawl_protected_page(url)
        results.append(data)
        time.sleep(2)  # 添加延时

    # 保存结果
    with open('protected_data.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=4)

    print(f'Total {len(results)} pages saved to protected_data.json')

if __name__ == '__main__':
    main()
```

### 8.4 分布式爬虫案例

以下是一个使用Scrapy和Redis实现的分布式爬虫示例：

首先，安装必要的库：

```bash
pip install scrapy scrapy-redis redis
```

创建Scrapy项目：

```bash
scrapy startproject distributed_crawler
cd distributed_crawler
scrapy genspider example example.com
```

修改设置文件 `distributed_crawler/settings.py`：

```python
# 启用Redis调度器
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

# 确保所有爬虫共享相同的去重指纹
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

# Redis连接设置
REDIS_URL = 'redis://localhost:6379'

# 可选：设置为True允许暂停/恢复爬取
SCHEDULER_PERSIST = True

# 可选：使用优先级队列（默认）
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'

# 默认请求序列化程序是pickle，但也可以使用json
# SCHEDULER_SERIALIZER = "scrapy_redis.picklecompat"

# 可选：设置Redis客户端的其他参数
# REDIS_HOST = 'localhost'
# REDIS_PORT = 6379

# 可选：指定Redis数据库索引
# REDIS_DB = 0

# 可选：指定Redis密码
# REDIS_PASSWORD = 'password'

# 默认项目管道
ITEM_PIPELINES = {
    'distributed_crawler.pipelines.DistributedCrawlerPipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 400,
}

# 爬虫允许的最大并发请求数
CONCURRENT_REQUESTS = 16

# 下载延迟
DOWNLOAD_DELAY = 1

# 启用Cookie
COOKIES_ENABLED = True

# 启用自动限速
AUTOTHROTTLE_ENABLED = True
AUTOTHROTTLE_START_DELAY = 5
AUTOTHROTTLE_MAX_DELAY = 60
```

创建Redis爬虫 `distributed_crawler/spiders/example.py`：

```python
import scrapy
from scrapy_redis.spiders import RedisSpider
from distributed_crawler.items import ProductItem

class ExampleSpider(RedisSpider):
    name = 'example'
    redis_key = 'example:start_urls'

    # 不需要定义start_urls，它们将从Redis获取

    def parse(self, response):
        # 解析产品列表页
        for product in response.css('div.product'):
            # 提取产品信息
            item = ProductItem()
            item['name'] = product.css('h2.title::text').get()
            item['price'] = product.css('span.price::text').get()
            item['url'] = product.css('a.product-link::attr(href)').get()

            # 如果URL是相对路径，转为绝对路径
            if item['url'] and not item['url'].startswith('http'):
                item['url'] = response.urljoin(item['url'])

            # 跟进产品详情页
            yield scrapy.Request(
                url=item['url'],
                callback=self.parse_product,
                meta={'item': item}
            )

        # 跟进下一页
        next_page = response.css('a.next-page::attr(href)').get()
        if next_page:
            yield scrapy.Request(url=response.urljoin(next_page), callback=self.parse)

    def parse_product(self, response):
        # 从meta获取之前的item
        item = response.meta['item']

        # 提取详细信息
        item['description'] = response.css('div.description::text').get()
        item['image_urls'] = response.css('div.product-images img::attr(src)').getall()
        item['category'] = response.css('ul.breadcrumb li:nth-child(2)::text').get()

        yield item
```

定义Item `distributed_crawler/items.py`：

```python
import scrapy

class ProductItem(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    url = scrapy.Field()
    description = scrapy.Field()
    image_urls = scrapy.Field()
    category = scrapy.Field()
```

创建Pipeline `distributed_crawler/pipelines.py`：

```python
import json
from itemadapter import ItemAdapter

class DistributedCrawlerPipeline:
    def process_item(self, item, spider):
        # 数据清洗和转换
        adapter = ItemAdapter(item)

        # 清理价格字段
        if adapter.get('price'):
            # 移除货币符号和空白字符，转换为浮点数
            price_str = adapter['price'].strip().replace('$', '').replace(',', '')
            try:
                adapter['price'] = float(price_str)
            except ValueError:
                adapter['price'] = None

        # 清理描述字段
        if adapter.get('description'):
            adapter['description'] = adapter['description'].strip()

        return item
```

运行爬虫的方式：

1. 首先启动Redis服务器
2. 在不同的机器或进程中启动爬虫：

```bash
scrapy runspider distributed_crawler/spiders/example.py
```

3. 向Redis添加起始URL：

```python
import redis

r = redis.Redis(host='localhost', port=6379, db=0)
r.lpush('example:start_urls', 'https://example.com/products')
```

这样，多个爬虫实例可以同时工作，共享同一个URL队列和去重过滤器，实现分布式爬取。

## 9. 爬虫项目优化

### 9.1 性能优化

爬虫性能优化的关键点：

1. **并发与异步**

```python
import asyncio
import aiohttp
import time

async def fetch(url, session):
    async with session.get(url) as response:
        return await response.text()

async def main():
    urls = [
        'https://example.com/page1',
        'https://example.com/page2',
        'https://example.com/page3',
        # 更多URL...
    ]

    start_time = time.time()

    async with aiohttp.ClientSession() as session:
        tasks = [fetch(url, session) for url in urls]
        results = await asyncio.gather(*tasks)

    end_time = time.time()
    print(f'Downloaded {len(urls)} pages in {end_time - start_time:.2f} seconds')

# 运行异步函数
asyncio.run(main())
```

2. **连接池复用**

```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# 创建会话对象
session = requests.Session()

# 配置重试策略
retry_strategy = Retry(
    total=3,  # 最大重试次数
    backoff_factor=1,  # 重试间隔因子
    status_forcelist=[429, 500, 502, 503, 504]  # 需要重试的HTTP状态码
)

# 配置连接池
adapter = HTTPAdapter(
    max_retries=retry_strategy,
    pool_connections=10,  # 连接池大小
    pool_maxsize=10  # 最大连接数
)

# 挂载适配器
session.mount('http://', adapter)
session.mount('https://', adapter)

# 使用会话对象发送请求
response = session.get('https://example.com')
```

3. **使用生产者-消费者模式**

```python
import threading
import queue
import requests
import time

# 创建队列
url_queue = queue.Queue()
result_queue = queue.Queue()

# 生产者函数：生成URL
def producer():
    urls = [
        'https://example.com/page1',
        'https://example.com/page2',
        # 更多URL...
    ]

    for url in urls:
        url_queue.put(url)

# 消费者函数：爬取URL
def consumer():
    while True:
        # 从队列获取URL，如果队列为空且所有生产者都已完成，则退出
        try:
            url = url_queue.get(timeout=1)
        except queue.Empty:
            break

        try:
            response = requests.get(url, timeout=10)
            result_queue.put((url, response.text))
        except Exception as e:
            result_queue.put((url, str(e)))
        finally:
            url_queue.task_done()
            time.sleep(1)  # 控制爬取速度

# 主函数
def main():
    # 启动生产者
    producer_thread = threading.Thread(target=producer)
    producer_thread.start()

    # 启动多个消费者
    consumers = []
    for _ in range(5):  # 5个消费者线程
        consumer_thread = threading.Thread(target=consumer)
        consumer_thread.daemon = True  # 设为守护线程
        consumer_thread.start()
        consumers.append(consumer_thread)

    # 等待生产者完成
    producer_thread.join()

    # 等待所有URL被处理
    url_queue.join()

    # 处理结果
    results = []
    while not result_queue.empty():
        results.append(result_queue.get())

    print(f'Crawled {len(results)} pages')

if __name__ == '__main__':
    main()
```

### 9.2 代码结构优化

良好的代码结构可以提高爬虫的可维护性和可扩展性：

```
my_crawler/
├── config/
│   ├── __init__.py
│   ├── settings.py        # 配置参数
│   └── logging_config.py  # 日志配置
├── crawler/
│   ├── __init__.py
│   ├── downloader.py      # 下载器
│   ├── parser.py          # 解析器
│   └── scheduler.py       # 调度器
├── database/
│   ├── __init__.py
│   ├── models.py          # 数据模型
│   └── storage.py         # 存储接口
├── utils/
│   ├── __init__.py
│   ├── url_utils.py       # URL处理工具
│   └── user_agents.py     # User-Agent工具
├── main.py                # 入口文件
└── requirements.txt       # 依赖列表
```

### 9.3 异常处理

健壮的爬虫需要完善的异常处理机制：

```python
import requests
import time
import logging
from requests.exceptions import RequestException, Timeout, TooManyRedirects

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filename='crawler.log'
)
logger = logging.getLogger('crawler')

def safe_request(url, max_retries=3, timeout=10):
    """安全的请求函数，包含重试和异常处理"""
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, timeout=timeout)
            response.raise_for_status()  # 如果状态码不是200，抛出异常
            return response
        except Timeout:
            logger.warning(f"Timeout occurred when requesting {url}, retry {retries+1}/{max_retries}")
        except TooManyRedirects:
            logger.error(f"Too many redirects for {url}")
            break  # 不再重试
        except RequestException as e:
            logger.warning(f"Request error for {url}: {e}, retry {retries+1}/{max_retries}")

        # 增加重试间隔
        time.sleep(2 ** retries)  # 指数退避
        retries += 1

    logger.error(f"Failed to request {url} after {max_retries} retries")
    return None

# 使用示例
url = 'https://example.com'
try:
    response = safe_request(url)
    if response:
        # 处理响应
        print(f"Successfully retrieved {url}, status code: {response.status_code}")
    else:
        print(f"Failed to retrieve {url}")
except Exception as e:
    logger.exception(f"Unexpected error when processing {url}: {e}")
```

### 9.4 日志记录

完善的日志系统对于监控和调试爬虫至关重要：

```python
import logging
from logging.handlers import RotatingFileHandler
import os
import time

def setup_logger(name, log_file, level=logging.INFO):
    """设置日志记录器"""
    # 创建日志目录
    log_dir = os.path.dirname(log_file)
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    # 创建处理器
    handler = RotatingFileHandler(
        log_file,
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )

    # 设置格式
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)

    # 创建日志记录器
    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)

    # 添加控制台输出
    console = logging.StreamHandler()
    console.setLevel(level)
    console.setFormatter(formatter)
    logger.addHandler(console)

    return logger

# 创建不同的日志记录器
info_logger = setup_logger('info', 'logs/info.log', logging.INFO)
error_logger = setup_logger('error', 'logs/error.log', logging.ERROR)
download_logger = setup_logger('download', 'logs/download.log', logging.INFO)

# 使用示例
def crawl_url(url):
    start_time = time.time()
    info_logger.info(f"Start crawling {url}")

    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()

        download_time = time.time() - start_time
        download_logger.info(f"Downloaded {url} in {download_time:.2f}s, size: {len(response.content)} bytes")

        return response.text
    except Exception as e:
        error_logger.error(f"Error crawling {url}: {e}", exc_info=True)
        return None
    finally:
        info_logger.info(f"Finished crawling {url} in {time.time() - start_time:.2f}s")
```

## 10. 爬虫的商业应用

### 10.1 数据采集与分析

爬虫可以用于收集和分析各种数据，如市场趋势、竞争对手信息、用户评价等。

**案例：电商价格监控系统**

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# 商品信息
products = [
    {'name': 'Product A', 'url': 'https://example.com/product1', 'target_price': 199.99},
    {'name': 'Product B', 'url': 'https://example.com/product2', 'target_price': 299.99},
    # 更多商品...
]

# 提取价格函数
def extract_price(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        # 根据网站结构调整选择器
        price_element = soup.select_one('span.price')
        if price_element:
            # 提取价格文本并转换为数字
            price_text = price_element.text.strip().replace('$', '').replace(',', '')
            return float(price_text)
    except Exception as e:
        print(f"Error extracting price from {url}: {e}")

    return None

# 发送邮件通知
def send_price_alert(product, current_price):
    sender_email = "your_email@gmail.com"
    receiver_email = "recipient@example.com"
    password = "your_password"  # 建议使用应用专用密码

    message = MIMEMultipart()
    message["From"] = sender_email
    message["To"] = receiver_email
    message["Subject"] = f"Price Alert: {product['name']} is now ${current_price}"

    body = f"The price of {product['name']} has dropped below your target price of ${product['target_price']}!\n\nCurrent price: ${current_price}\n\nProduct URL: {product['url']}"
    message.attach(MIMEText(body, "plain"))

    try:
        server = smtplib.SMTP("smtp.gmail.com", 587)
        server.starttls()
        server.login(sender_email, password)
        server.sendmail(sender_email, receiver_email, message.as_string())
        print(f"Price alert sent for {product['name']}")
    except Exception as e:
        print(f"Error sending email: {e}")
    finally:
        server.quit()

# 主函数
def monitor_prices():
    results = []

    for product in products:
        print(f"Checking price for {product['name']}...")
        current_price = extract_price(product['url'])

        if current_price:
            result = {
                'name': product['name'],
                'url': product['url'],
                'target_price': product['target_price'],
                'current_price': current_price,
                'is_deal': current_price <= product['target_price']
            }
            results.append(result)

            # 如果价格低于目标价格，发送通知
            if result['is_deal']:
                send_price_alert(product, current_price)

        # 添加延时避免请求过快
        time.sleep(2)

    # 保存结果到CSV
    if results:
        df = pd.DataFrame(results)
        df.to_csv('price_monitoring.csv', index=False)
        print(f"Saved {len(results)} results to price_monitoring.csv")

if __name__ == '__main__':
    monitor_prices()
```

### 10.2 搜索引擎

搜索引擎是爬虫最广泛的应用之一，它们使用爬虫来索引互联网上的网页。

**简单搜索引擎架构**：

1. **爬虫模块**：负责抓取网页
2. **索引模块**：处理和存储网页内容
3. **检索模块**：根据用户查询返回相关结果

### 10.3 舆情监控

爬虫可以用于监控社交媒体、新闻网站等，分析公众对特定话题、品牌或产品的情感和态度。

**案例：社交媒体情感分析**

```python
import requests
import pandas as pd
import time
import re
from textblob import TextBlob
import matplotlib.pyplot as plt

# 假设我们有一个API来获取社交媒体数据
def get_social_media_posts(keyword, days=7):
    # 这里应该是实际的API调用
    # 为了示例，我们模拟一些数据
    posts = [
        {"text": "I love the new product from Company X!", "date": "2023-01-01", "platform": "Twitter"},
        {"text": "The service was terrible, would not recommend.", "date": "2023-01-01", "platform": "Facebook"},
        {"text": "Average experience, nothing special.", "date": "2023-01-02", "platform": "Twitter"},
        # 更多帖子...
    ]
    return posts

# 清理文本
def clean_text(text):
    # 移除URL
    text = re.sub(r'http\S+', '', text)
    # 移除用户名
    text = re.sub(r'@\w+', '', text)
    # 移除特殊字符
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)
    # 移除多余空格
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# 情感分析
def analyze_sentiment(text):
    blob = TextBlob(text)
    # 返回情感极性（-1到1之间，负面到正面）
    return blob.sentiment.polarity

# 主函数
def monitor_sentiment(keyword, days=7):
    print(f"Monitoring sentiment for '{keyword}' over the past {days} days...")

    # 获取社交媒体帖子
    posts = get_social_media_posts(keyword, days)

    # 分析每个帖子
    results = []
    for post in posts:
        clean_post = clean_text(post['text'])
        sentiment = analyze_sentiment(clean_post)

        results.append({
            'text': post['text'],
            'clean_text': clean_post,
            'date': post['date'],
            'platform': post['platform'],
            'sentiment': sentiment,
            'sentiment_category': 'positive' if sentiment > 0.1 else ('negative' if sentiment < -0.1 else 'neutral')
        })

    # 转换为DataFrame
    df = pd.DataFrame(results)

    # 按日期和平台分组，计算平均情感
    sentiment_by_date = df.groupby('date')['sentiment'].mean().reset_index()
    sentiment_by_platform = df.groupby('platform')['sentiment'].mean().reset_index()

    # 情感分布
    sentiment_distribution = df['sentiment_category'].value_counts()

    # 可视化
    plt.figure(figsize=(15, 10))

    # 按日期的情感趋势
    plt.subplot(2, 2, 1)
    plt.plot(sentiment_by_date['date'], sentiment_by_date['sentiment'])
    plt.title('Sentiment Trend by Date')
    plt.xlabel('Date')
    plt.ylabel('Average Sentiment')

    # 按平台的平均情感
    plt.subplot(2, 2, 2)
    plt.bar(sentiment_by_platform['platform'], sentiment_by_platform['sentiment'])
    plt.title('Average Sentiment by Platform')
    plt.xlabel('Platform')
    plt.ylabel('Average Sentiment')

    # 情感分布饼图
    plt.subplot(2, 2, 3)
    plt.pie(sentiment_distribution, labels=sentiment_distribution.index, autopct='%1.1f%%')
    plt.title('Sentiment Distribution')

    plt.tight_layout()
    plt.savefig('sentiment_analysis.png')
    print(f"Saved sentiment analysis visualization to sentiment_analysis.png")

    # 保存结果
    df.to_csv('sentiment_results.csv', index=False)
    print(f"Saved {len(df)} analyzed posts to sentiment_results.csv")

    # 返回摘要
    summary = {
        'total_posts': len(df),
        'average_sentiment': df['sentiment'].mean(),
        'positive_percentage': (df['sentiment_category'] == 'positive').mean() * 100,
        'neutral_percentage': (df['sentiment_category'] == 'neutral').mean() * 100,
        'negative_percentage': (df['sentiment_category'] == 'negative').mean() * 100
    }

    return summary

if __name__ == '__main__':
    summary = monitor_sentiment('Company X')
    print("\nSentiment Analysis Summary:")
    for key, value in summary.items():
        print(f"{key}: {value}")
```

### 10.4 价格比较

爬虫可以用于收集不同电商平台的价格信息，帮助消费者找到最优惠的价格。

**案例：多平台价格比较**

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import concurrent.futures
import time

# 定义要搜索的产品
product_name = "iPhone 13 Pro 128GB"

# 定义不同电商平台的搜索URL和解析函数
platforms = [
    {
        'name': 'Amazon',
        'search_url': f'https://www.amazon.com/s?k={product_name.replace(" ", "+")}',
        'parser': lambda soup: {
            'price': soup.select_one('.a-price .a-offscreen').text.strip() if soup.select_one('.a-price .a-offscreen') else 'N/A',
            'title': soup.select_one('.a-size-medium.a-color-base.a-text-normal').text.strip() if soup.select_one('.a-size-medium.a-color-base.a-text-normal') else 'N/A',
            'link': 'https://www.amazon.com' + soup.select_one('.a-link-normal.a-text-normal')['href'] if soup.select_one('.a-link-normal.a-text-normal') else 'N/A'
        }
    },
    {
        'name': 'eBay',
        'search_url': f'https://www.ebay.com/sch/i.html?_nkw={product_name.replace(" ", "+")}',
        'parser': lambda soup: {
            'price': soup.select_one('.s-item__price').text.strip() if soup.select_one('.s-item__price') else 'N/A',
            'title': soup.select_one('.s-item__title').text.strip() if soup.select_one('.s-item__title') else 'N/A',
            'link': soup.select_one('.s-item__link')['href'] if soup.select_one('.s-item__link') else 'N/A'
        }
    },
    # 可以添加更多平台...
]

# 爬取单个平台
def crawl_platform(platform):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        response = requests.get(platform['search_url'], headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        # 找到第一个产品结果
        result = platform['parser'](soup)
        result['platform'] = platform['name']

        return result
    except Exception as e:
        print(f"Error crawling {platform['name']}: {e}")
        return {'platform': platform['name'], 'price': 'Error', 'title': 'Error', 'link': 'Error'}

# 主函数
def compare_prices():
    print(f"Comparing prices for '{product_name}'...")

    results = []

    # 使用线程池并行爬取
    with concurrent.futures.ThreadPoolExecutor(max_workers=len(platforms)) as executor:
        future_to_platform = {executor.submit(crawl_platform, platform): platform for platform in platforms}

        for future in concurrent.futures.as_completed(future_to_platform):
            platform = future_to_platform[future]
            try:
                result = future.result()
                results.append(result)
                print(f"Found on {platform['name']}: {result['price']} - {result['title']}")
            except Exception as e:
                print(f"Error processing {platform['name']}: {e}")

    # 转换为DataFrame并排序
    if results:
        df = pd.DataFrame(results)

        # 清理价格数据用于排序
        def extract_price(price_str):
            if price_str == 'N/A' or price_str == 'Error':
                return float('inf')  # 将无效价格放在最后
            # 提取数字部分
            import re
            price_num = re.sub(r'[^\d.]', '', price_str)
            try:
                return float(price_num)
            except:
                return float('inf')

        # 添加数值价格列用于排序
        df['price_value'] = df['price'].apply(extract_price)
        df = df.sort_values('price_value')

        # 删除辅助列
        df = df.drop('price_value', axis=1)

        # 保存结果
        df.to_csv('price_comparison.csv', index=False)
        print(f"\nSaved price comparison results to price_comparison.csv")

        # 显示最低价格
        if len(df) > 0 and df.iloc[0]['price'] != 'Error' and df.iloc[0]['price'] != 'N/A':
            print(f"\nBest price found: {df.iloc[0]['price']} on {df.iloc[0]['platform']}")
            print(f"Product: {df.iloc[0]['title']}")
            print(f"Link: {df.iloc[0]['link']}")
    else:
        print("No results found")

if __name__ == '__main__':
    compare_prices()
```

以上就是爬虫从基础到实战的全面指南，涵盖了爬虫的基本概念、工作原理、常用库、技术难点、实战案例以及商业应用。通过学习和实践这些内容，你可以掌握爬虫技术，并将其应用到实际项目中。
result = response.json()
if result.get('success') or result.get('token'):
    print('Login successful')

    # 如果返回了token，在后续请求中使用
    if 'token' in result:
        headers['Authorization'] = f'Bearer {result["token"]}'
    
    # 访问需要登录的API
    protected_api = session.get('https://api.example.com/protected', headers=headers)
    print(protected_api.json())

else:
    print('Login failed:', result.get('message'))

```
### 5.4 代理IP的使用

使用代理IP可以避免IP被封禁，分散请求压力。

1. **使用单个代理**

```python
import requests

proxy = {
    'http': 'http://user:pass@10.10.10.10:8080',
    'https': 'http://user:pass@10.10.10.10:8080'
}

response = requests.get('https://www.example.com', proxies=proxy)
print(response.text)
```

2. **使用代理池**

```python
import requests
import random

# 代理池
proxy_list = [
    {'http': 'http://10.10.10.1:8080', 'https': 'http://10.10.10.1:8080'},
    {'http': 'http://10.10.10.2:8080', 'https': 'http://10.10.10.2:8080'},
    {'http': 'http://10.10.10.3:8080', 'https': 'http://10.10.10.3:8080'},
    # 更多代理...
]

# 随机选择代理
proxy = random.choice(proxy_list)

response = requests.get('https://www.example.com', proxies=proxy)
print(response.text)
```

3. **使用代理服务**

```python
import requests

# 从代理服务获取代理
proxy_api = 'http://proxy-provider.com/api/proxy?api_key=YOUR_API_KEY'
response = requests.get(proxy_api)
proxy_data = response.json()

proxy = {
    'http': f'http://{proxy_data["ip"]}:{proxy_data["port"]}',
    'https': f'http://{proxy_data["ip"]}:{proxy_data["port"]}'
}

response = requests.get('https://www.example.com', proxies=proxy)
print(response.text)
```

### 5.5 User-Agent池

轮换使用不同的User-Agent可以模拟不同的浏览器，减少被识别为爬虫的可能性。

```python
import requests
import random

# User-Agent池
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
    # 更多User-Agent...
]

# 随机选择User-Agent
headers = {
    'User-Agent': random.choice(user_agents)
}

response = requests.get('https://www.example.com', headers=headers)
print(response.text)
```

### 5.6 分布式爬虫

分布式爬虫可以利用多台机器的资源，提高爬取效率。常用的分布式爬虫框架和工具有：

1. **Scrapy + Redis**

```python
# settings.py
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
REDIS_URL = 'redis://redis-server:6379'
REDIS_START_URLS_AS_SET = True

# spider.py
from scrapy_redis.spiders import RedisSpider

class MySpider(RedisSpider):
    name = 'myspider'
    redis_key = 'myspider:start_urls'

    def parse(self, response):
        # 解析逻辑
        pass
```

2. **Celery + RabbitMQ**

```python
# tasks.py
from celery import Celery
import requests
from bs4 import BeautifulSoup

app = Celery('tasks', broker='pyamqp://guest@localhost//')

@app.task
def crawl(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # 解析逻辑
    title = soup.title.text

    # 提取新的URL并创建新任务
    for link in soup.find_all('a'):
        href = link.get('href')
        if href and href.startswith('http'):
            crawl.delay(href)

    return {'url': url, 'title': title}
```

3. **自定义分布式系统**

```python
# master.py
import redis
import json

# 连接Redis
r = redis.Redis(host='localhost', port=6379, db=0)

# 添加初始URL到队列
initial_urls = [
    'https://example.com/page1',
    'https://example.com/page2',
    # 更多URL...
]

for url in initial_urls:
    r.lpush('crawl_queue', url)

# worker.py
import redis
import requests
from bs4 import BeautifulSoup
import json
import time

# 连接Redis
r = redis.Redis(host='localhost', port=6379, db=0)

while True:
    # 从队列获取URL
    url = r.rpop('crawl_queue')
    if not url:
        time.sleep(1)
        continue

    url = url.decode('utf-8')

    try:
        # 爬取页面
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')

        # 解析数据
        title = soup.title.text if soup.title else ''

        # 存储结果
        result = {'url': url, 'title': title, 'timestamp': time.time()}
        r.lpush('crawl_results', json.dumps(result))

        # 提取新的URL
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and href.startswith('http'):
                # 检查URL是否已爬取
                if not r.sismember('crawled_urls', href):
                    r.lpush('crawl_queue', href)
                    r.sadd('crawled_urls', href)

    except Exception as e:
        # 记录错误
        error = {'url': url, 'error': str(e), 'timestamp': time.time()}
        r.lpush('crawl_errors', json.dumps(error))
```

## 6. 反爬虫与反反爬技术

### 6.1 常见的反爬虫机制

网站常用的反爬虫机制包括：

1. **基于请求频率的限制**：限制同一IP在一定时间内的请求次数
2. **基于User-Agent的检测**：识别爬虫特有的User-Agent
3. **基于Cookie/Session的验证**：要求请求携带有效的Cookie
4. **验证码**：要求用户输入验证码
5. **JavaScript验证**：使用JavaScript生成必要的参数或Token
6. **蜜罐陷阱**：设置对普通用户不可见但爬虫可能访问的链接
7. **IP黑名单**：封禁被识别为爬虫的IP
8. **内容加密**：对关键内容进行加密或混淆

### 6.2 请求频率控制

控制请求频率可以避免触发网站的反爬机制。

1. **固定延时**

```python
import requests
import time

urls = [
    'https://example.com/page1',
    'https://example.com/page2',
    # 更多URL...
]

for url in urls:
    response = requests.get(url)
    print(f'Crawled {url}')

    # 固定延时2秒
    time.sleep(2)
```

2. **随机延时**

```python
import requests
import time
import random

urls = [
    'https://example.com/page1',
    'https://example.com/page2',
    # 更多URL...
]

for url in urls:
    response = requests.get(url)
    print(f'Crawled {url}')

    # 随机延时1-5秒
    time.sleep(random.uniform(1, 5))
```

3. **指数退避算法**

```python
import requests
import time
import random

def crawl_with_exponential_backoff(url, max_retries=5):
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, timeout=10)
            if response.status_code == 200:
                return response
            elif response.status_code == 429:  # Too Many Requests
                # 计算等待时间
                wait_time = (2 ** retries) + random.uniform(0, 1)
                print(f'Rate limited. Waiting {wait_time:.2f} seconds...')
                time.sleep(wait_time)
                retries += 1
            else:
                print(f'Error: {response.status_code}')
                return None
        except requests.exceptions.RequestException as e:
            print(f'Request failed: {e}')
            return None

    print('Max retries reached')
    return None

urls = [
    'https://example.com/page1',
    'https://example.com/page2',
    # 更多URL...
]

for url in urls:
    response = crawl_with_exponential_backoff(url)
    if response:
        print(f'Successfully crawled {url}')
```

### 6.3 Cookie与Session处理

许多网站使用Cookie来跟踪用户状态和防止爬虫。

1. **使用Session对象保持Cookie**

```python
import requests

# 创建会话对象
session = requests.Session()

# 首次访问，获取Cookie
response = session.get('https://example.com')

# 后续请求会自动带上之前的Cookie
response = session.get('https://example.com/page1')
response = session.get('https://example.com/page2')
```

2. **手动设置Cookie**

```python
import requests

# 从浏览器中复制Cookie
cookies = {
    'session_id': 'abc123',
    'user_id': '456',
    'csrf_token': 'xyz789'
}

# 使用Cookie发送请求
response = requests.get('https://example.com', cookies=cookies)
```

3. **处理需要JavaScript的Cookie**

```python
from selenium import webdriver
import time
import requests

# 使用Selenium获取Cookie
driver = webdriver.Chrome()
driver.get('https://example.com')

# 等待JavaScript执行
time.sleep(3)

# 获取所有Cookie
selenium_cookies = driver.get_cookies()

# 转换为requests可用的格式
cookies = {}
for cookie in selenium_cookies:
    cookies[cookie['name']] = cookie['value']

driver.quit()

# 使用获取的Cookie发送请求
session = requests.Session()
session.cookies.update(cookies)
response = session.get('https://example.com/protected')
```

### 6.4 JavaScript混淆的应对

一些网站使用JavaScript混淆来保护数据或生成必要的参数。

1. **使用Selenium执行JavaScript**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.get('https://example.com')

# 执行JavaScript获取数据
data = driver.execute_script("""
    // 这里是JavaScript代码
    return document.querySelector('.hidden-data').textContent;
""")

print(data)
driver.quit()
```

2. **使用PyExecJS执行JavaScript**

```python
import execjs
import requests

# 获取混淆的JavaScript代码
response = requests.get('https://example.com')
html = response.text

# 提取JavaScript代码
# 假设我们已经从HTML中提取出了JavaScript代码
js_code = """function generateToken(timestamp) { return 'token_' + timestamp; }"""

# 创建JavaScript运行环境
ctx = execjs.compile(js_code)

# 执行JavaScript函数
timestamp = int(time.time())
token = ctx.call('generateToken', timestamp)

# 使用生成的token发送请求
params = {'timestamp': timestamp, 'token': token}
response = requests.get('https://example.com/api', params=params)
```

3. **使用专门的JavaScript引擎**

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# 配置Chrome以无头模式运行
options = Options()
options.add_argument('--headless')
options.add_argument('--disable-gpu')

driver = webdriver.Chrome(options=options)
driver.get('https://example.com')

# 等待JavaScript执行
driver.implicitly_wait(5)

# 获取JavaScript处理后的数据
data = driver.find_element(By.ID, 'data-container').text

driver.quit()
```

### 6.5 验证码绕过技术

除了前面提到的验证码识别方法，还有一些绕过验证码的技术：

1. **使用验证码识别服务**

```python
import requests
from PIL import Image
from io import BytesIO

# 获取验证码图片
session = requests.Session()
response = session.get('https://example.com/login')

# 假设我们从响应中提取了验证码图片URL
captcha_url = 'https://example.com/captcha.php'
response = session.get(captcha_url)

# 保存验证码图片
image = Image.open(BytesIO(response.content))
image.save('captcha.png')

# 使用验证码识别服务
with open('captcha.png', 'rb') as f:
    files = {'image': f}
    api_response = requests.post('https://api.captcha-service.com/recognize', 
                               files=files, 
                               data={'api_key': 'YOUR_API_KEY'})

captcha_text = api_response.json()['text']

# 使用识别结果登录
login_data = {
    'username': 'your_username',
    'password': 'your_password',
    'captcha': captcha_text
}

login_response = session.post('https://example.com/login', data=login_data)
```

2. **寻找API绕过验证码**

```python
import requests
import json

# 有些网站的移动端API可能没有验证码

api_url = 'https://api.example.com/login'
headers = {
    'User-Agent': 'MobileApp/1.0',
    'Content-Type': 'application/json'
}

login_data = {
    'username': 'your_username',
    'password': 'your_password'
}