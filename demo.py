#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
çˆ¬è™«ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬
å±•ç¤ºåŸºç¡€çˆ¬è™«å’Œé«˜çº§çˆ¬è™«çš„åŠŸèƒ½
"""

import os
import time
import json
from news_crawler_basic import BasicNewsCrawler
from news_crawler_advanced import AdvancedNewsCrawler

def demo_basic_crawler():
    """
    æ¼”ç¤ºåŸºç¡€çˆ¬è™«åŠŸèƒ½
    """
    print("\n" + "="*60)
    print("ğŸš€ åŸºç¡€çˆ¬è™«æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºåŸºç¡€çˆ¬è™«å®ä¾‹
    crawler = BasicNewsCrawler()
    
    print("ğŸ“° å¼€å§‹çˆ¬å–æ–°é—»...")
    
    # çˆ¬å–æ–°é—»
    news_data = crawler.crawl(['news'], max_pages=2)
    
    if news_data:
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(news_data)} æ¡æ–°é—»")
        print("\nğŸ“‹ å‰3æ¡æ–°é—»é¢„è§ˆ:")
        for i, news in enumerate(news_data[:3], 1):
            print(f"\n{i}. {news['title'][:50]}...")
            print(f"   ğŸ•’ æ—¶é—´: {news['pub_time']}")
            print(f"   ğŸ”— é“¾æ¥: {news['link'][:60]}...")
            if news['summary']:
                print(f"   ğŸ“ æ‘˜è¦: {news['summary'][:80]}...")
    else:
        print("âŒ æœªèƒ½è·å–åˆ°æ–°é—»æ•°æ®")
    
    return news_data

def demo_advanced_crawler():
    """
    æ¼”ç¤ºé«˜çº§çˆ¬è™«åŠŸèƒ½
    """
    print("\n" + "="*60)
    print("ğŸ”¥ é«˜çº§çˆ¬è™«æ¼”ç¤º")
    print("="*60)
    
    # é…ç½®é«˜çº§çˆ¬è™«
    config = {
        'max_workers': 3,
        'request_delay': (1, 2),
        'timeout': 15,
        'max_retries': 3,
        'use_proxy': False,
        'target_sites': [
            {
                'name': 'ç½‘æ˜“æ–°é—»',
                'base_url': 'https://news.163.com/',
                'list_selector': 'a',
                'title_selector': 'h1',
                'content_selector': '.post_content_main, .post_text'
            }
        ]
    }
    
    # åˆ›å»ºé«˜çº§çˆ¬è™«å®ä¾‹
    crawler = AdvancedNewsCrawler(config)
    
    print("ğŸ¤– å¯åŠ¨é«˜çº§çˆ¬è™«ï¼ˆå¤šçº¿ç¨‹ + æƒ…æ„Ÿåˆ†æ + æ•°æ®å¯è§†åŒ–ï¼‰...")
    
    # è¿è¡Œçˆ¬è™«
    crawler.run(max_news_per_site=20)
    
    print(f"\nâœ… é«˜çº§çˆ¬è™«å®Œæˆï¼Œå…±çˆ¬å– {len(crawler.news_data)} æ¡æ–°é—»")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if os.path.exists('news_data/statistics.json'):
        with open('news_data/statistics.json', 'r', encoding='utf-8') as f:
            stats = json.load(f)
        
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   ğŸ“° æ€»æ–°é—»æ•°: {stats.get('total_news', 0)}")
        print(f"   ğŸ“ å¹³å‡å­—æ•°: {stats.get('avg_word_count', 0):.0f}")
        print(f"   ğŸ’­ å¹³å‡æƒ…æ„Ÿåˆ†: {stats.get('avg_sentiment', 0):.3f}")
        
        if 'sources' in stats:
            print("   ğŸŒ æ–°é—»æ¥æº:")
            for source, count in stats['sources'].items():
                print(f"      - {source}: {count}æ¡")
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    data_files = [
        'news_data/news.db',
        'news_data/news_export.csv',
        'news_data/news_export.xlsx',
        'news_data/statistics.json',
        'charts/news_sources.png',
        'charts/sentiment_distribution.png',
        'wordclouds/keywords_wordcloud.png'
    ]
    
    for file_path in data_files:
        if os.path.exists(file_path):
            print(f"   âœ… {file_path}")
        else:
            print(f"   âŒ {file_path} (æœªç”Ÿæˆ)")
    
    return crawler.news_data

def show_system_info():
    """
    æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    """
    print("\n" + "="*60)
    print("ğŸ“‹ æ™ºèƒ½æ–°é—»çˆ¬è™«ç³»ç»Ÿä¿¡æ¯")
    print("="*60)
    
    features = [
        "ğŸ”¸ åŸºç¡€çˆ¬è™«: å¿«é€Ÿæ–°é—»é‡‡é›†",
        "ğŸ”¸ é«˜çº§çˆ¬è™«: å¤šçº¿ç¨‹ + æƒ…æ„Ÿåˆ†æ + å¯è§†åŒ–",
        "ğŸ”¸ Webç®¡ç†ç•Œé¢: å®æ—¶ç›‘æ§å’Œæ§åˆ¶",
        "ğŸ”¸ æ•°æ®å­˜å‚¨: CSV, JSON, Excel, SQLite",
        "ğŸ”¸ æƒ…æ„Ÿåˆ†æ: ä¸­æ–‡æ–‡æœ¬æƒ…æ„Ÿå€¾å‘åˆ†æ",
        "ğŸ”¸ å…³é”®è¯æå–: åŸºäºjiebaåˆ†è¯çš„TF-IDF",
        "ğŸ”¸ æ•°æ®å¯è§†åŒ–: å›¾è¡¨å’Œè¯äº‘ç”Ÿæˆ",
        "ğŸ”¸ æ™ºèƒ½åçˆ¬: User-Agentè½®æ¢ + ä»£ç†æ”¯æŒ",
        "ğŸ”¸ æ–­ç‚¹ç»­çˆ¬: é¿å…é‡å¤çˆ¬å–",
        "ğŸ”¸ å®æ—¶ç›‘æ§: Webç•Œé¢çŠ¶æ€å±•ç¤º"
    ]
    
    print("\nğŸš€ æ ¸å¿ƒåŠŸèƒ½:")
    for feature in features:
        print(f"   {feature}")
    
    print("\nğŸŒ Webç•Œé¢è®¿é—®:")
    print("   ğŸ“± æœ¬åœ°è®¿é—®: http://127.0.0.1:5000")
    print("   ğŸ–¥ï¸  ç½‘ç»œè®¿é—®: http://192.168.10.121:5000")
    
    print("\nğŸ“š ä½¿ç”¨æ–¹æ³•:")
    print("   1. è¿è¡Œ python crawler_manager.py å¯åŠ¨WebæœåŠ¡")
    print("   2. æµè§ˆå™¨è®¿é—® http://127.0.0.1:5000")
    print("   3. åœ¨Webç•Œé¢ä¸­æ§åˆ¶çˆ¬è™«å’ŒæŸ¥çœ‹æ•°æ®")
    
    print("\nâš ï¸  æ³¨æ„äº‹é¡¹:")
    print("   â€¢ è¯·éµå®ˆç½‘ç«™robots.txtåè®®")
    print("   â€¢ æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›")
    print("   â€¢ ä»…ç”¨äºå­¦ä¹ ç ”ç©¶ç›®çš„")

def main():
    """
    ä¸»æ¼”ç¤ºå‡½æ•°
    """
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½æ–°é—»çˆ¬è™«ç³»ç»Ÿæ¼”ç¤º")
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    show_system_info()
    
    # è¯¢é—®ç”¨æˆ·é€‰æ‹©
    print("\n" + "="*60)
    print("è¯·é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. åŸºç¡€çˆ¬è™«æ¼”ç¤º")
    print("2. é«˜çº§çˆ¬è™«æ¼”ç¤º")
    print("3. å®Œæ•´æ¼”ç¤ºï¼ˆåŸºç¡€ + é«˜çº§ï¼‰")
    print("4. ä»…æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯")
    print("="*60)
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == '1':
            demo_basic_crawler()
        elif choice == '2':
            demo_advanced_crawler()
        elif choice == '3':
            print("\nğŸ¯ å¼€å§‹å®Œæ•´æ¼”ç¤º...")
            demo_basic_crawler()
            time.sleep(2)
            demo_advanced_crawler()
        elif choice == '4':
            print("\nâœ… ç³»ç»Ÿä¿¡æ¯å·²æ˜¾ç¤º")
        else:
            print("\nâŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¿è¡Œç¨‹åº")
            return
        
        print("\n" + "="*60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ’¡ æç¤º: è¿è¡Œ 'python crawler_manager.py' å¯åŠ¨Webç•Œé¢")
        print("ğŸŒ ç„¶åè®¿é—® http://127.0.0.1:5000 æŸ¥çœ‹å®Œæ•´åŠŸèƒ½")
        print("="*60)
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ¼”ç¤ºå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == '__main__':
    main()